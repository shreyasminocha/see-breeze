# -*- coding: utf-8 -*-
"""NDBC_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mmYPwcMdZkuLPG5O0zW8A_PXK_rnt2ro
"""

!pip install wget
!pip install pandas

import wget
import pandas as pd
import numpy as np

from datetime import date
from sklearn.linear_model import LinearRegression

import matplotlib.pyplot as plt

#url = 'https://www.ndbc.noaa.gov/data/realtime2/KIKT.txt'
url = 'https://www.ndbc.noaa.gov/view_text_file.php?filename=42375h2014.txt.gz&dir=data/historical/stdmet/'
#filename = wget.download(url)
#print(filename)

fileurl = '42375h2014'

import tarfile

# tar = tarfile.open('/42375h2014.txt.gz')
# tar.extractall()

# !unzip content/42375h2014.txt.gz

# import urllib
# txt = urllib.urlopen(url).read()

#!wget -r -A --content-disposition '*.txt' 'https://www.ndbc.noaa.gov/view_text_file.php?filename=42375h2014.txt.gz&dir=data/historical/stdmet/'

ff = open(fileurl+'_NEW'+'.txt','w')
with open(fileurl+'.txt') as f:
    lines = f.readlines()
    lines.pop(1)
    ff.writelines(lines)
#for i in range(len(lines)):
# for i in range(10):
#   print(lines[i])

#data=pd.read_csv('KIKT2.txt', sep='\s+', header=None)

df = pd.read_table(fileurl+'_NEW'+'.txt',sep="\s+")

df

#df = df.replace('MM', np.NaN)

#~df.columns.isin(['MM','DD'])

#df.columns

#df.columns[~df.columns.isin(['MM','DD'])]

#df = df.replace(99.0, np.NaN)

#df.at[500, '#YY']

df

# REPLACE ALL 9's

#df = df.loc[:, df.columns != 'MM' && df.columns != 'DD'].replace(9.0, np.NaN)
#df['ATMP']
df2 = df[df.columns[~df.columns.isin(['MM','DD'])]].replace(9.0, np.NaN)
df2 = df2.assign(MM = lambda x: df['MM'])
df = df.assign(DD = lambda x: df['DD'])

df

# REPLACE ALL OTHER 9 VALUES (99, 999, 999, etc)

df = df.replace(99.0, np.NaN)
df = df.replace(999.0, np.NaN)
df = df.replace(9999.0, np.NaN)

df

#df['hh'][500]

#df.head()

df = df[df['WDIR'].notna()]
df = df[df['WSPD'].notna()]

#10367 rows
df = df[df['WVHT'].notna()]
df = df[df['DPD'].notna()]
# 10236 rows

df

# Reset the index (must reset index before iterating over rows). This will make a new 'index' column
# that represents the old (not necessarily consecutive) indexing.
df = df.reset_index()

df = df.drop(['GST', 'APD', 'MWD', 'PRES', 'ATMP', 'WTMP', 'DEWP', 'VIS', 'TIDE'], axis=1)

df

# CREATE A NEW COLUMN CALLED DAYS_SINCE_START_OF_YEAR WHICH RECORDS THE NUMBER OF DAYS
# SINCE THE START OF THE YEAR.
for index, row in df.iterrows():
    ##print(row['WDIR'], row['WSPD'])
    #print("Cur Date")
    #print(row['#YY'], row['MM'], row['DD'])
    since_date = date(int(row['#YY']), 1, 1)
    cur_date = date(int(row['#YY']), int(row['MM']), int(row['DD']))
    diff_date = cur_date - since_date
    num_days = diff_date.days
    num_hours = int(row['hh']) + int(row['mm'])/60
    if (index < 100):
      print(row['hh'], row['mm'], num_hours)
    num_days += num_hours/24
    #print(num_days)

    df.at[index, 'Days_Since_Start_of_Year'] = num_days

#df['Days_since_start_of_year'] = (date(int(df['#YY']), 1, 1) - date(int(df['#YY']), int(df['MM']), int(df['DD']))).days

df

DF_BY_INDEX = 70/10

df.iloc[1, :]

df[df.index % DF_BY_INDEX == 0].iloc[31, :]

df_new = df[df.index % DF_BY_INDEX == 0]
df_new = df_new.reset_index()

df_new

def create_dataset(X, y, look_back=1):
    dataX, dataY = [], []
    for i in range(X.shape[0]-look_back):
        a = X.iloc[i:(i+look_back), :].values.flatten()
        dataX.append(a)
        dataY.append(y.iloc[i])

    return np.array(dataX), np.array(dataY)

rows_shift = 45*10
#rows_shift = 3
X = df_new[['Days_Since_Start_of_Year', 'WVHT', 'DPD']]
y = df_new['WSPD'].shift(-rows_shift)
X_np, y_np = create_dataset(X, y, look_back=rows_shift)
cols_per_row = 3
col_index = 0
X_shape = X_np.shape
print("Shape OF X_np=", X_shape)

while col_index < X_shape[1]:
  print(X_np[0][col_index:col_index+cols_per_row])
  col_index += cols_per_row

print("X_np index")
print(df_new.iloc[45*10 - 1])

X.iloc[0:(0+450), :].values

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

#X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.20, random_state=42)
X_shape
train_index = int(0.70*X_shape[0])
test_index = int((X_shape[0] - train_index)/2)
val_index = test_index
print("X_shape=", X_shape)
X_train, y_train = X_np[0:train_index], y_np[0:train_index]
X_test, y_test = X_np[train_index:train_index+test_index], y_np[train_index:train_index+test_index]
X_val, y_val = X_np[train_index+test_index:], y_np[train_index+test_index:]

print(X_train[0][1347])

#print(X_test.shape)
print(X_test[:, 1347])

reg = LinearRegression()
reg.fit(X_train,y_train)

reg.coef_

y_test_pred = reg.predict(X_test)

print(y_test_pred[115], y_test[115])

np.sqrt(np.mean((y_test_pred-y_test)**2))

plt.plot(X_test[:, 1347], y_test, 'o')
plt.plot(X_test[:, 1347], y_test_pred, 'o', color='black')

# NEW SEGMENT OF CODE WHICH DEALS WITH LSTM

def create_dataset_without_flatten(X, y, look_back=1):
    dataX, dataY = [], []
    for i in range(X.shape[0]-look_back):
        a = X.iloc[i:(i+look_back), :].values
        #print("a=", a)
        dataX.append(a)
        dataY.append(y.iloc[i])

    return np.array(dataX), np.array(dataY)

rows_shift = 45*10
#rows_shift = 3
X = df_new[['Days_Since_Start_of_Year', 'WVHT', 'DPD']]
y = df_new['WSPD'].shift(-rows_shift)
X_np, y_np = create_dataset_without_flatten(X, y, look_back=rows_shift)
cols_per_row = 3
col_index = 0
X_shape = X_np.shape

# X_np.shape # (1022, 450, 3)
# X_shape
train_index = int(0.70*X_shape[0])
test_index = int((X_shape[0] - train_index)/2)
val_index = test_index
print("X_shape=", X_shape)
X_train, y_train = X_np[0:train_index], y_np[0:train_index]
X_test, y_test = X_np[train_index:train_index+test_index], y_np[train_index:train_index+test_index]
X_val, y_val = X_np[train_index+test_index:], y_np[train_index+test_index:]

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam
tf.config.run_functions_eagerly(True)

model1 = Sequential()
model1.add(InputLayer((450, 3)))
model1.add(LSTM(64))
model1.add(Dense(8, 'relu'))
model1.add(Dense(1, 'linear'))

model1.summary()

cp1 = ModelCheckpoint('model1/', save_best_only=True)
model1.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.005), metrics=[RootMeanSquaredError()])

model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[cp1])

model1.save('dir_for_model1')

from tensorflow.keras.models import load_model
model2 = load_model('dir_for_model1/')

model1.layers[1].weights

#train_predictions = model1.predict(X_train)
#.flatten()
#print(train_predictions)
#train_results = pd.DataFrame(data={'Train Predictions':train_predictions, 'Actuals':y_train})
#train_results

# train_predictions = model2.predict(X_train)

# print(np.sqrt(np.mean((train_predictions-y_train)**2)))
# print(len(train_predictions))
# print(len(y_train))

test_predictions = model2.predict(X_test)

print(np.sqrt(np.mean((test_predictions-y_test)**2)))
print(len(test_predictions))
print(len(y_test))


# val_predictions = model2.predict(X_val)

# print(np.sqrt(np.mean((val_predictions-y_val)**2)))
# print(len(val_predictions))
# print(len(y_val))

# DON'T RUN CODE BELOW

# X = df.drop(['WSPD'],axis=1).values
# y = df['WSPD'].values

# X.shape

# y.shape

# X_train, X_test, y_train, y_test = train_test_split(X, y)

# X_train.shape

# LR = LinearRegression()
# LR.fit

# DON'T RUN THIS CELL
# FROM: https://stackoverflow.com/questions/59368923/linearregression-by-given-last-n-rows

#import numpy as np
#import pandas as pd
#import matplotlib.pyplot as plt
#from sklearn.linear_model import LinearRegression

# generate random data to test

if False:
  df4 = pd.DataFrame(np.random.normal(size=(2000, 4)))
  df4.columns = ['Open', 'High', 'Low', 'Close']

  days = 10
  df4 = df4.dropna()

  total = len(df4)
  test_ratio = 0.30
  test_size = int(total * test_ratio)
  X = df4[['Open', 'High', 'Low', 'Close']]

  # Now the row 1 of df['Close'] will have the output 'Close' value
  # for what was originally row 11
  # In other words, y is shifted 10 rows up
  y = df4['Close'].shift(-days)

  # this function based on the MachineLearningMastery page mentioned 
  def create_dataset(X, y, look_back=1):
      dataX, dataY = [], []
      for i in range(X.shape[0]-look_back):
          a = X.iloc[i:(i+look_back), :].values.flatten()
          dataX.append(a)
          dataY.append(y.iloc[i])

      return np.array(dataX), np.array(dataY)

  #build test and train data
  X_train, y_train = create_dataset(X[:-test_size], y[:-test_size], look_back=days)
  X_test, y_test = create_dataset(X[-test_size:], y[-test_size:], look_back=days)
  # build model
  lr = LinearRegression()
  lr.fit(X_train, y_train)
  y_pred = lr.predict(X_test)
  plt.scatter(y_pred, y_test)
  plt.show()
  print("X_train before=")
  print(X[:-test_size].iloc[0:20, :])
  print(X_train)
  print(y_train)

# df4 = pd.DataFrame(np.random.normal(size=(10, 4)))
# print(df4)
# df4 = df4.shift(-5)
# print(df4)